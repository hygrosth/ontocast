# ===========================================
# OntoCast Environment Configuration
# Copy this file to .env and update with your values
# ===========================================

# Domain Configuration
# Used for URI generation in the knowledge graph
CURRENT_DOMAIN=https://example.com

# Server Configuration
PORT=8999
RECURSION_LIMIT=1000
ESTIMATED_CHUNKS=30
MAX_VISITS=3

# LLM Configuration
# Choose your LLM provider and configure accordingly

# OpenAI Configuration (Default)
LLM_PROVIDER=openai
LLM_MODEL_NAME=gpt-4o-mini
LLM_TEMPERATURE=0.0
LLM_API_KEY=your_openai_api_key_here

# Ollama Configuration (Alternative)
# LLM_PROVIDER=ollama
# LLM_MODEL_NAME=granite3.3
# LLM_BASE_URL=http://localhost:11434
# LLM_TEMPERATURE=0.0

# Triple Store Configuration
# Fuseki is preferred over Neo4j when both are configured
# If no triple store is configured, OntoCast will use filesystem storage

# Fuseki Configuration (Recommended)
FUSEKI_URI=http://localhost:3030/test
FUSEKI_AUTH=admin/admin
FUSEKI_DATASET=dataset_name

# Neo4j Configuration (Alternative)
NEO4J_URI=bolt://localhost:7687
NEO4J_AUTH=neo4j/test
NEO4J_PORT=7476
NEO4J_BOLT_PORT=7689

# Path Configuration
# WORKING_DIRECTORY=/path/to/working/directory
# ONTOLOGY_DIRECTORY=/path/to/ontology/files

# Additional Configuration
# CLEAN=false  # Set to true to clean triple store on startup
# SKIP_ONTOLOGY_DEVELOPMENT=false  # Set to true to skip ontology critique
# LOGGING_LEVEL=info  # Set logging level (debug, info, warning, error) 